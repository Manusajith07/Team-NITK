# -*- coding: utf-8 -*-
"""WCE_Bleeding_Classification_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J5Il59AKOzbYJKlwnucfWmRjS0rGQ47Y
"""

#LOAD DEPENDENCIES
import os
import time
import pickle
import logging
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

# from sklearn.utils import class_weight
from tensorflow.keras.models import *
from tensorflow.keras.optimizers import *

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import *
from tensorflow.keras.layers import *

from google.colab import drive
 drive.mount('/content/drive')

#LOAD THE DATA
train_data_dir = "/content/drive/MyDrive/WCEBleedDataset/Train_New"
img_rows, img_cols = 224, 224
input_shape = (img_rows,img_cols,3)
model_input = Input(shape=input_shape)
print("Data folders found!")
print("The Input size is set to ", model_input)

batch_size = 4
train_datagen = ImageDataGenerator(rescale=1./255,
      horizontal_flip=True,
      vertical_flip=True,
      rotation_range=45,
      shear_range=0.1,
      zoom_range=0.1,
      height_shift_range=0.1,
      width_shift_range=0.1,
      fill_mode='constant',
      brightness_range=[0.5, 1.0],
      validation_split=0.2) # set validation split

train_generator = train_datagen.flow_from_directory(
    directory=train_data_dir,
    target_size=(img_rows, img_cols),
    batch_size=batch_size,
    class_mode='binary',
    subset='training') # set as training data

validation_generator = train_datagen.flow_from_directory(
    directory=train_data_dir, # same directory as training data
    target_size=(img_rows, img_cols),
    batch_size=batch_size,
    class_mode='binary',
    subset='validation') # set as validation data

def Dense_with_SE_block(inp,nf,s):
    inp =  Conv2D(nf, 1, padding='same', strides=s,kernel_initializer='he_normal')(inp)
    c1 = Conv2D(nf,5,strides=s,padding='same')(inp)
    c1 = BatchNormalization()(c1)
    c1 = Activation('relu')(c1)
    c1 = Concatenate()([c1,inp])
    c2 = Conv2D(nf,3,strides=s,padding='same')(c1)
    c2 = BatchNormalization()(c2)
    c2 = Activation('relu')(c2)
    c2 = Concatenate()([c2,c1,inp])                          ### Dense connection with different kernel sizes which can effectively extract multiscale feactures keeping the previous layer info.
    c3 = Conv2D(nf,1,strides=s,padding='same')(c2)
    c3 = BatchNormalization()(c3)
    c3 = Activation('relu')(c3)
    c3 = Concatenate()([c3,c2,c1,inp])
    c3 =  Conv2D(nf, 1, padding='same', strides=s,kernel_initializer='he_normal')(c3)
    gap = GlobalAveragePooling2D()(c3)
    dense1 = Dense(nf,activation='relu')(gap)
    dense2 = Dense(nf,activation='sigmoid')(dense1)
    c = Multiply()([c3,dense2])    ## giving more attention to the important features
    c = Add()([c,inp])
    return c

def classify(x):
    gap = GlobalAveragePooling2D()(x)
    dense4 = Dense(1,activation='sigmoid')(gap)
    return dense4

def bhc(shape):
  inp = Input(shape)
  conv1 = Conv2D(16,3,padding='same',activation='relu')(inp)
  se1 = Dense_with_SE_block(conv1,16,1)
  se1 = Dense_with_SE_block(se1,16,1)

  max1 = MaxPooling2D((2,2))(se1)
  se2 = Dense_with_SE_block(max1,32,1)
  se2 = Dense_with_SE_block(se2,32,1)

  max2 = MaxPooling2D((2,2))(se2)
  se3 = Dense_with_SE_block(max2,64,1)
  se3 = Dense_with_SE_block(se3,64,1)

  out = classify(se3)
  model = Model(inp, out)
  return model

BHCNet = bhc((224,224,3))

def get_pr(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def get_re(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

import tensorflow.keras.backend as K
def get_f1(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return precision,recall,f1_val

BHCNet.summary()

BHCNet.compile(optimizer='Adam', loss='binary_crossentropy',metrics=['accuracy',get_pr,get_re,get_f1])

#### Training for 60 epochs. Used learning rate scheduler.

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy' , mode='max' ,
                                                  factor = 0.5 , patience = 8 , verbose=1 , cooldown = 1,
                                                 min_delta = 0.0001)

early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=30, verbose=1,
                                              mode = 'max', restore_best_weights = True)
check_path = '/content/drive/MyDrive/New_weights/New_weight-2.h5'
checkpoint = tf.keras.callbacks.ModelCheckpoint(check_path, monitor = 'val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='max')


history = BHCNet.fit(train_generator, validation_data=validation_generator, batch_size=4,shuffle=True, verbose=1
                                  ,epochs = 40, callbacks = [reduce_lr, early_stop, checkpoint] )

BHCNet.load_weights('/content/drive/MyDrive/New_weights/New_weight-2.h5')

import os
pred = []
files = []
path = '/content/drive/MyDrive/WCEBleedDataset/Test/Test Dataset 2/'
for filename in os.listdir(path):
    img = plt.imread(os.path.join(path, filename))# images are color images
    filename = filename.split(".")[0]
    img1 = np.expand_dims(img,axis=0)
    pr = BHCNet.predict(img1)
    pr1 = np.where(pr>0.5,1,0)
    files.append(filename)
    pred.append(pr1)

pred1 = np.array(pred)

pred1 = np.reshape(pred1,len(pred1))

import pandas as pd
results_table1 = pd.DataFrame(np.c_[files,pred1], columns=["Filenames", "Prediction"])
print(results_table1)

results_table1['Prediction'].replace('0', 'Non-Bleeding', inplace=True)

results_table1['Prediction'].replace('1', 'Bleeding', inplace=True)

results_table1.to_csv('/content/drive/MyDrive/WCEBleedDataset/Test/Test_Dataset2.csv')